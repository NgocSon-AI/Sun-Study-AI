{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "932022c7",
   "metadata": {},
   "source": [
    "# Semantic Clustering\n",
    "In this notebook we'll see how we can use pre-trained language models to embed text into fixed length vectors. The models we will use have been trained to produce embedding vectors that are close by if the input text has a similar semantic content! This can be useful for comparing chunks of text and performing vector based search!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e84e255",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from tqdm.notebook import trange, tqdm\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from torchtext.datasets import AG_NEWS, IMDB\n",
    "\n",
    "# Make sure you are using the lastest version!\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "torch.backends.cuda.matmul.allow_tf32 = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d70b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Approx number of text samples to use\n",
    "num_data_points = 10000\n",
    "\n",
    "# Define the batch size for mini-batch gradient descent\n",
    "batch_size = 64\n",
    "\n",
    "# https://www.kaggle.com/datasets/ltcmdrdata/plain-text-wikipedia-202011\n",
    "# Define the root directory of the dataset\n",
    "data_set_root = \"../../datasets\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3be3989",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if GPU is available, set device accordingly\n",
    "device = torch.device(1 if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c383fcb4",
   "metadata": {},
   "source": [
    "## Data processing and Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3bd6c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subtract 1 from all labels to make them 0 and 1 (not 1 and 2...)\n",
    "# make everything lowercase\n",
    "def process_data(x):\n",
    "    return x[0] - 1, x[1].lower(), \n",
    "\n",
    "dataset_train = IMDB(root=data_set_root, split=\"train\")\n",
    "dataset_test = IMDB(root=data_set_root, split=\"test\")\n",
    "    \n",
    "dataset_train = dataset_train.map(process_data)\n",
    "dataset_test = dataset_test.map(process_data)\n",
    "\n",
    "# IMDB does not seem to be properly shuffled....\n",
    "dataset_train = dataset_train.shuffle(buffer_size=10000).set_shuffle(True)\n",
    "dataset_test = dataset_test.shuffle(buffer_size=10000).set_shuffle(True)\n",
    "\n",
    "# This is a hack to get around some random bug with the IMDB dataset not properly\n",
    "# Processing the positive (pos) datapoints, you only need to do this once...\n",
    "# This will take a few seconds..\n",
    "for label, text in dataset_train:\n",
    "    continue\n",
    "    \n",
    "for label, text in dataset_test:\n",
    "    continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac4652f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loaders for the training and testing datasets\n",
    "data_loader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True, num_workers=8)\n",
    "data_loader_test = DataLoader(dataset_test, batch_size=batch_size, shuffle=True, num_workers=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02bfb27b",
   "metadata": {},
   "source": [
    "## Create embedding model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68fcf20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model from HuggingFace Hub\n",
    "tokenizer = AutoTokenizer.from_pretrained('BAAI/bge-small-en-v1.5')\n",
    "model = AutoModel.from_pretrained('BAAI/bge-small-en-v1.5').to(device)\n",
    "_ = model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15d0a36",
   "metadata": {},
   "source": [
    "## Extract embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7a5f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_log = []\n",
    "labels_log = []\n",
    "text_log = []\n",
    "\n",
    "# Loop over each batch in the training dataset\n",
    "for label, text in tqdm(data_loader_train, desc=\"Extracting\", leave=False, total=num_data_points//batch_size):\n",
    "    encoded_input = tokenizer(text, padding=True, truncation=True, return_tensors='pt').to(device)\n",
    "\n",
    "    with torch.cuda.amp.autocast():\n",
    "        with torch.no_grad():\n",
    "            embeddings = model(**encoded_input)[0][:, 0]\n",
    "            \n",
    "            norm_embeddings = torch.nn.functional.normalize(embeddings, p=2, dim=1)\n",
    "            \n",
    "            embeddings_log.append(norm_embeddings.cpu())\n",
    "            labels_log.append(label)\n",
    "            text_log += list(text)\n",
    "            \n",
    "    if len(labels_log) * batch_size >= num_data_points:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2462d973",
   "metadata": {},
   "outputs": [],
   "source": [
    "np_embeddings = torch.cat(embeddings_log).numpy()\n",
    "np_labels = torch.cat(labels_log).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7556775c",
   "metadata": {},
   "source": [
    "## Cluster the Embedddings!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9cd816",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = 2  # You can adjust this number based on your data\n",
    "kmeans = KMeans(n_clusters=n_clusters)\n",
    "cluster_labels = kmeans.fit_predict(np_embeddings)\n",
    "cluster_centers = kmeans.cluster_centers_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "269ee7ec",
   "metadata": {},
   "source": [
    "## Perform dimension reduction for visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a0f1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA Dim reduction\n",
    "pca = PCA(n_components=20)\n",
    "\n",
    "# Stack embs and centers to project together\n",
    "combined_embs = np.vstack([np_embeddings, cluster_centers])\n",
    "combined_pca = pca.fit_transform(combined_embs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd824df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TSNE Dim reduction (Maintain local distances)\n",
    "tsne = TSNE(n_components=2, perplexity=50)\n",
    "combined_2d = tsne.fit_transform(combined_pca)\n",
    "\n",
    "# Separate the projected embeddings and centers\n",
    "embeddings_2d = combined_2d[:-n_clusters]\n",
    "centers_2d = combined_2d[-n_clusters:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff878bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a scatter plot\n",
    "plt.figure(figsize=(8, 5))\n",
    "\n",
    "cluster_0_index = np.where(cluster_labels == 0)[0]\n",
    "scatter1 = plt.scatter(embeddings_2d[cluster_0_index, 0], embeddings_2d[cluster_0_index, 1], \n",
    "                      c=np_labels[cluster_0_index], s=5, marker=\"_\")\n",
    "\n",
    "cluster_1_index = np.where(cluster_labels == 1)[0]\n",
    "scatter2 = plt.scatter(embeddings_2d[cluster_1_index, 0], embeddings_2d[cluster_1_index, 1], \n",
    "                      c=np_labels[cluster_1_index], s=5, marker=\"o\")\n",
    "\n",
    "_ = plt.scatter(centers_2d[:, 0], centers_2d[:, 1], c=\"r\", s=100, marker=\"x\")\n",
    "_ = plt.legend([\"cluster 0\", \"cluster 1\"])\n",
    "_ = plt.xlabel('t-SNE feature 1')\n",
    "_ = plt.ylabel('t-SNE feature 2')\n",
    "_ = plt.title('t-SNE visualization of embeddings with Semantic label')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6cb0bf6",
   "metadata": {},
   "source": [
    "## Find outlier reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25727e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the indices of all points in cluster 0\n",
    "cluster_0_indices = np.where(cluster_labels == 0)[0]\n",
    "\n",
    "# Get the labels and embeddings of points in cluster 0\n",
    "cluster_0_labels = np_labels[cluster_0_indices]\n",
    "cluster_0_points = np_embeddings[cluster_0_indices]\n",
    "\n",
    "# Find the most common semantic label for this cluster\n",
    "cluster_0_median_label = np.median(cluster_0_labels)\n",
    "print(\"The most common semantic label is %d\" % cluster_0_median_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac740e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.hist(cluster_0_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1d7abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the cluster indices of points within this cluster that do not have the typical semantic label\n",
    "outlier_indices = np.where(~(cluster_0_labels == cluster_0_median_label))[0]\n",
    "\n",
    "# Get the origional indices for these points (to index text list)\n",
    "cluster_0_outlier_indices = cluster_0_indices[outlier_indices]\n",
    "\n",
    "# Get the embeddings of the outliers\n",
    "cluster_0_outlier_points = cluster_0_points[outlier_indices]\n",
    "cluster_0_outlier_labels = cluster_0_labels[outlier_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cfdc598",
   "metadata": {},
   "source": [
    "## Find the worst outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1304a81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Find the distance between each outlier embedding and the cluster center for cluster 0\n",
    "points_diff = (cluster_0_outlier_points - cluster_centers[0].reshape(1, -1))\n",
    "points_dist = np.mean(np.power(points_diff, 2), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d73c870",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the outlier that is closest to the cluster center\n",
    "# AKA the \"worst\" outlier\n",
    "closest_5 = np.argsort(points_dist)[:5]\n",
    "closest_5_indices = cluster_0_outlier_indices[closest_5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba9a0fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "closest_5_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5486df57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the origional text for this outlier\n",
    "outlier_text = text_log[closest_5_indices[0]]\n",
    "outlier_label = np_labels[closest_5_indices[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2245f863",
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd356815",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
